{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from sklearn.tree import plot_tree\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "random_state = 1\n",
    "train_size = 0.75\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from file (or url) and save the dataframe\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "df = pd.read_csv(url, sep = ';') \n",
    "# if the names of the columns are not present, insert them using `names = []`\n",
    "# if the file is an excel use df = pd.read_excel(data_fn)\n",
    "print(f\"Shape of the input data {df.shape}\")\n",
    "\n",
    "target = 'quality'\n",
    "\n",
    "# storing in X the content of the dataframe excluding the target column\n",
    "X = df.drop(target, axis=1)\n",
    "# storing in y the labels\n",
    "y = df[target]\n",
    "print(f\"Shape of X: {X.shape}\\nShape of y: {y.shape}\")\n",
    "\n",
    "# dividing the dataset in train and test\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=random_state, train_size = train_size)\n",
    "print(\"There are {} samples in the training dataset\".format(Xtrain.shape[0]))\n",
    "print(\"There are {} samples in the testing dataset\".format(Xtest.shape[0]))\n",
    "print(\"Each sample has {} features\".format(Xtrain.shape[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the p-values of the target with respect to the variables\n",
    "_, p_values = f_regression(X,y)\n",
    "p_values_show = pd.DataFrame({'Variable': X.columns, 'p-value': p_values})\n",
    "p_values_show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the model\n",
    "model = DecisionTreeClassifier(criterion = 'entropy')\n",
    "# fitting the model\n",
    "model.fit(Xtrain, ytrain)\n",
    "\n",
    "# using the model to predict the labels of the training set\n",
    "ytrain_model = model.predict(Xtrain)   \n",
    "accuracy_train = accuracy_score(ytrain, ytrain_model)\n",
    "print(f\"The accuracy on training set is {(accuracy_train * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the model to predict the label of new data\n",
    "ytest_model = model.predict(Xtest)\n",
    "accuracy_test = accuracy_score(ytest, ytest_model)\n",
    "print(\"The accuracy on test set is {0:.2f}%\".format(accuracy_test * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the decision tree\n",
    "figure(figsize = (15,15))\n",
    "plot_tree(model\n",
    "#          , fontsize=6\n",
    "          , filled=True\n",
    "          , feature_names = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "          , class_names = ['setosa', 'versicolor', 'virginica']\n",
    "          , rounded = True\n",
    "          , proportion = True\n",
    "         );"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the estimator to predict the label of new data\n",
    "y_predicted_test = model.predict(Xtest)\n",
    "accuracy_test = accuracy_score(ytest, y_predicted_test) * 100\n",
    "fitted_max_depth = model.tree_.max_depth\n",
    "initial_impurity = model.tree_.impurity[0] # the impurity variable of tree_ contains the impurities of all the nodes\n",
    "print(f\"The accuracy on test set is {accuracy_test:.1f}%\")\n",
    "print(f\"The maximum depth of the tree fitted on X_train is {fitted_max_depth}\")\n",
    "# print(\"The impurity of the X_train dataset is {0:.3f}\".format(initial_impurity))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_values = range(1,fitted_max_depth+1)\n",
    "# Tuning with cross validation\n",
    "# we'll build an estimator changing the depth of the decision tree\n",
    "# we'll compute the scores and we'll save the average in a list\n",
    "avg_scores = []\n",
    "for par in parameter_values:\n",
    "    estimator = DecisionTreeClassifier(criterion=\"entropy\"\n",
    "                                            , max_depth = par\n",
    "                                            , random_state = random_state\n",
    "                                            )\n",
    "    scores = cross_val_score(estimator, Xtrain, ytrain\n",
    "                             , scoring='accuracy', cv = 5)\n",
    "    # cross_val_score produces an array with one score for each fold\n",
    "    avg_scores.append(np.mean(scores))\n",
    "print(avg_scores)\n",
    "\n",
    "plt.figure(figsize=(32,20))\n",
    "plt.plot(parameter_values, avg_scores, '-o', linewidth=5, markersize=24)\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title(\"Score with Cross Validation varying max_depth of tree\", fontsize = 24)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the depth of the tree that obtained the best result\n",
    "top_par_cv = parameter_values[np.argmax(avg_scores)]\n",
    "# create an estimator using the best depth\n",
    "estimator = DecisionTreeClassifier(criterion=\"entropy\", max_depth = top_par_cv)\n",
    "estimator.fit(Xtrain,ytrain);\n",
    "y_predicted = estimator.predict(Xtest)\n",
    "accuracy_cv = accuracy_score(ytest, y_predicted) * 100\n",
    "print(f\"The accuracy on test set tuned with cross_validation is {accuracy_cv:.1f}% with depth {top_par_cv}\")\n",
    "\n",
    "# showing more infromation on the classifier\n",
    "print(classification_report(ytest, y_predicted))\n",
    "# printing the confusion matrix\n",
    "print(confusion_matrix(ytest, y_predicted))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using several classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. The model used is the support vector machine\n",
    "tuned_param_svc = [{'kernel': 'rbf', \n",
    "                    'C': [1, 10, 100, 1000],\n",
    "                    },\n",
    "                    {'kernel': 'linear',\n",
    "                     'C': [1, 10, 100, 1000],                     \n",
    "                    },\n",
    "                   ]\n",
    "\n",
    "avg_scores_2 = []\n",
    "for par in tuned_param_svc:\n",
    "    for c in par['C']:\n",
    "        estimator = SVC(kernel=par['kernel'], C=c)\n",
    "        scores = cross_val_score(estimator, Xtrain, ytrain\n",
    "                                , scoring='accuracy', cv = 5)\n",
    "        # cross_val_score produces an array with one score for each fold\n",
    "        avg_scores_2.append(np.mean(scores))\n",
    "print(avg_scores_2)\n",
    "\n",
    "best = np.argmax(avg_scores_2)\n",
    "best_param = {'kernel': tuned_param_svc[int(best/4)]['kernel'],\\\n",
    "    'C': tuned_param_svc[int(best/4)]['C'][best%4]}\n",
    "estimator_2 = SVC(**best_param)\n",
    "\n",
    "estimator_2.fit(Xtrain,ytrain);\n",
    "y_predicted = estimator_2.predict(Xtest)\n",
    "accuracy_cv = accuracy_score(ytest, y_predicted) * 100\n",
    "print(f\"The accuracy on test set tuned with cross_validation is {accuracy_cv:.1f}% using the kernel {estimator_2.kernel} and C={estimator_2.C}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (main, Aug 10 2022, 11:40:04) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
